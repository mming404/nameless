server:
  port: 8882

spring:
  application:
    name: order
  #数据源
  datasource:
    username: root
    password: 123456
    url: jdbc:mysql://10.21.32.168:3306/nameless_order?characterEncoding=utf8&serverTimezone=Asia/Shanghai&useSSL=false
    driver-class-name: com.mysql.cj.jdbc.Driver

  # shardingsphere 配置数据源，给数据源起名是ds0,ds1...此处可配置多数据源
  shardingsphere:
    mode:
      type: Standalone
    datasource:
      names: nameless_order
      nameless_order:
        type: com.zaxxer.hikari.HikariDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        jdbc-url: jdbc:mysql://10.21.32.168:3306/nameless_order?useUnicode=true&characterEncoding=utf8&autoReconnect=true&zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false
        username: root
        password: 123456

    rules:
      sharding:
        tables:
          t_order:
            actual-data-nodes: nameless_order.t_order_$->{0..1}
            table-strategy:
              standard:
                sharding-column: id
                sharding-algorithm-name: order-inline
            key-generate-strategy:
              column: id
              key-generator-name: snowflake
        key-generators:
          snowflake:
            type: SNOWFLAKE
        sharding-algorithms:
          order-inline:
            type: INLINE
            props:
              algorithm-expression: t_order_$->{id % 2}
    props:
      sql-show: true

  data:
    redis:
      host: 119.91.222.238
      port: 6379
      password: 123456
      database: 0
      timeout: 5000
      lettuce:
        pool:
          max-active: 8
          max-idle: 30
          max-wait: 10000
          min-idle: 10
  # 卡夫卡配置
  kafka:
    bootstrap-servers: 10.21.32.168:9092
    # 生产者
    producer:
      retries: 3 # 设置大于 0 的值，则客户端会将发送失败的记录重新发送
      batch-size: 16384
      buffer-memory: 33554432
      acks: 1
      # 指定消息key和消息体的编解码方式
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
    # 消费者
    consumer:
      group-id: default-group
      enable-auto-commit: false
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      max-poll-records: 500
    listener:
      # 当每一条记录被消费者监听器（ListenerConsumer）处理之后提交
      # RECORD
      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后提交
      # BATCH
      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后，距离上次提交时间大于TIME时提交
      # TIME
      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后，被处理record数量大于等于COUNT时提交
      # COUNT
      # TIME | COUNT　有一个条件满足时提交
      # COUNT_TIME
      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后, 手动调用Acknowledgment.acknowledge()后提交
      # MANUAL
      # 手动调用Acknowledgment.acknowledge()后立即提交，一般使用这种
      # MANUAL_IMMEDIATE
      ack-mode: MANUAL_IMMEDIATE

dubbo:
  protocol:
    name: dubbo
    port: -1
    serialization: fastjson2
  registry:
    address: nacos://10.21.32.86:8848?username=nacos&password=nacos

#mp配置
mybatis-plus:
  mapper-locations: classpath*:/mapper/**/*.xml
  #配置逻辑删除字段
  global-config:
    db-config:
      logic-delete-field: deleted
  #sql打印
  configuration:
    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl