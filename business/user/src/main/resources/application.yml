server:
  port: 8884

spring:
  application:
    name: user

  #数据源
  datasource:
    username: root
    password: 123456
    url: jdbc:mysql://10.21.32.168:3306/nameless_user?useSSL=false&characterEncoding=utf8&serverTimezone=Asia/Shanghai&useSSL=false
    driver-class-name: com.mysql.cj.jdbc.Driver
    type: com.alibaba.druid.pool.DruidDataSource
    druid:
      initial-size: 10
      max-active: 60
      min-idle: 5
      #设置防火墙 监控
      filters: wall,mergeStat
      filter:
        #sql监控设置
        stat:
          #是否记录慢查询
          log-slow-sql: true
          #慢查询的sql时间  毫秒
          slow-sql-millis: 5000
      #监控信息页面 用户密码
      stat-view-servlet:
        login-username: diversion-system
        login-password: diversion-system-backend
        #默认开启
        enabled: true
        allow:
        deny:
      test-while-idle: true
      test-on-borrow: true

  #redis集群配置
  data:
#    redis:
#      host: 119.91.222.238
#      port: 6379
#      password: 123456
#      database: 0
#      timeout: 5000
#      lettuce:
#        pool:
#          max-active: 8
#          max-idle: 30
#          max-wait: 10000
#          min-idle: 10
    redis:
      password: hthredis
      cluster:
        nodes: 10.21.32.86:6371,10.21.32.86:6372,10.21.32.39:6373,10.21.32.39:6374,10.21.32.147:6375,10.21.32.147:6376
        max-redirects: 3
      lettuce:
        pool:
          min-idle: 0
          max-idle: 8
          max-wait: -1ms
          max-active: 8
        cluster:
          refresh:
            adaptive: true
            period: 2000



  # 卡夫卡配置
  kafka:
    bootstrap-servers: 10.21.32.168:9092

    # 生产者
    producer:
      retries: 3 # 设置大于 0 的值，则客户端会将发送失败的记录重新发送
      batch-size: 16384
      buffer-memory: 33554432
      acks: 1
      # 指定消息key和消息体的编解码方式
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      # 消息幂等 开启时acks要为all
#      properties:
#        enable.idempotence: true
    # 消费者
    consumer:
      group-id: default-group
      enable-auto-commit: false
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      max-poll-records: 50

    listener:
      # 当每一条记录被消费者监听器（ListenerConsumer）处理之后提交
      # RECORD
      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后提交
      # BATCH
      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后，距离上次提交时间大于TIME时提交
      # TIME
      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后，被处理record数量大于等于COUNT时提交
      # COUNT
      # TIME | COUNT　有一个条件满足时提交
      # COUNT_TIME
      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后, 手动调用Acknowledgment.acknowledge()后提交
      # MANUAL
      # 手动调用Acknowledgment.acknowledge()后立即提交，一般使用这种
      # MANUAL_IMMEDIATE
      ack-mode: MANUAL_IMMEDIATE
      type: batch

#dubbo配置
dubbo:
  protocol:
    name: dubbo
    port: -1
    serialization: fastjson2
  registry:
    address: nacos://10.21.32.86:8848

#mp配置
mybatis-plus:
  mapper-locations: classpath*:/mapper/**/*.xml
  #配置逻辑删除字段
  global-config:
    db-config:
      logic-delete-field: deleted
  #sql打印
  configuration:
    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl
